{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc696eda",
   "metadata": {},
   "source": [
    "# **Return of the Schema** for *Apulia Travel KG* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1f207",
   "metadata": {},
   "source": [
    "## Path Definition Basic Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, RDF, RDFS, OWL, Namespace\n",
    "from urllib.parse import quote\n",
    "from rdflib.namespace import split_uri\n",
    "from rdflib.term import URIRef\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import csv\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def serialize(graph, path):\n",
    "    graph.serialize(path.with_suffix(\".xml\"), format=\"xml\")\n",
    "    !/home/navis/robot/robot merge --input {path.with_suffix(\".xml\")} --output {path.with_suffix(\".owl\")}\n",
    "    path.with_suffix(\".xml\").unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATERIALIZE = True\n",
    "DATASET_NAME = \"APULIATRAVEL\"\n",
    "DATASET_NAME += f\"-{'MATERIALIZE' if MATERIALIZE else \"BASE\"}\"\n",
    "\n",
    "home_path = Path().cwd().absolute().parent.parent \n",
    "dataset_path = home_path / \"kgsaf_data\" / f\"{'materialize' if MATERIALIZE else \"base\"}\" / \"unpack\" / DATASET_NAME\n",
    "onto_path = home_path / \"kgsaf_data\" / \"ontologies\"/ \"unpack\" / \"APULIATRAVEL\"\n",
    "\n",
    "print(\"Base Path\", home_path)\n",
    "print(\"Ontology\", onto_path)\n",
    "print(\"Dataset\", dataset_path)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "if MATERIALIZE:\n",
    "    print(\"Loading MATERIALIZED Ontology\")\n",
    "    onto_file = onto_path / \"apulia_travel_merged_materialized.owl\"\n",
    "else:\n",
    "    print(\"Loading BASE Ontology\")\n",
    "    onto_file = onto_path / \"apulia_travel_merged.owl\"\n",
    "\n",
    "print(\"\\tLoading Ontology\")\n",
    "\n",
    "apulia_onto = Graph()\n",
    "apulia_onto.parse(onto_file)\n",
    "\n",
    "print(\"\\tOntology Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03536b2a",
   "metadata": {},
   "source": [
    "# [O] ABOX Triple Cleaning\n",
    "\n",
    "Removal of triples with individuals that are also classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbe278",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_triples = Graph()\n",
    "data_triples.parse(onto_path / \"ApuliaTravelABox.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = set(data_triples.predicates())\n",
    "len(predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = set()\n",
    "\n",
    "for pred in predicates:\n",
    "    if (pred, RDF.type, OWL.ObjectProperty) in apulia_ontology:\n",
    "        final_pred.add(pred)\n",
    "    else:\n",
    "        print(f\"Removing {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1036ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_graph = Graph()\n",
    "\n",
    "with open(onto_path / \"tsv_triples.tsv\", \"w\") as f:\n",
    "    for s,p,o in data_triples:\n",
    "        if (p in final_pred):\n",
    "            if (not ((s, RDF.type, OWL.Class) in apulia_ontology)) and (not ((o, RDF.type, OWL.Class) in apulia_ontology)):\n",
    "                out_graph.add((s,p,o))\n",
    "                f.write(f\"{str(s)}\\t{str(p)}\\t{str(o)}\\n\")\n",
    "\n",
    "out_graph.serialize(onto_path / \"apulia_clean_abox.nt\", format=\"nt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykeen\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.triples.splitting import CoverageSplitter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "triples = TriplesFactory.from_path(onto_path / \"tsv_triples.tsv\")\n",
    "\n",
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20021068",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mappings = {v:k for k,v in triples.entity_id_to_label.items()}\n",
    "relation_mappings = {v:k for k,v in triples.relation_id_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = triples.split(\n",
    "    ratios=[0.85, 0.05, 0.1],\n",
    "    random_state=42,\n",
    "    method=CoverageSplitter(),      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = TriplesFactory.from_labeled_triples(\n",
    "    triples=train.triples,\n",
    "    entity_to_id=entity_mappings,\n",
    "    relation_to_id=relation_mappings\n",
    ")\n",
    "\n",
    "valid_clean = TriplesFactory.from_labeled_triples(\n",
    "    triples=valid.triples,\n",
    "    entity_to_id=entity_mappings,\n",
    "    relation_to_id=relation_mappings\n",
    ")\n",
    "\n",
    "test_clean = TriplesFactory.from_labeled_triples(\n",
    "    triples=test.triples,\n",
    "    entity_to_id=entity_mappings,\n",
    "    relation_to_id=relation_mappings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_clean)\n",
    "print(test_clean)\n",
    "print(valid_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd919e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.triples.leakage import unleak\n",
    "\n",
    "train_unleak, valid_unleak, test_unleak = unleak(\n",
    "    train_clean,\n",
    "    *[valid_clean, test_clean],\n",
    "    n=None,\n",
    "    minimum_frequency=0.97\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e17915",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_unleak)\n",
    "print(test_unleak)\n",
    "print(valid_unleak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a713bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "    (dataset_path / \"abox/splits/train\", train_unleak.triples),\n",
    "    (dataset_path / \"abox/splits/valid\", valid_unleak.triples),\n",
    "    (dataset_path / \"abox/splits/test\", test_unleak.triples)\n",
    "]\n",
    "\n",
    "\n",
    "for path, split in targets:\n",
    "    out_graph = Graph()\n",
    "    for triple in split:\n",
    "        s = URIRef(triple[0])\n",
    "        p = URIRef(triple[1])\n",
    "        o = URIRef(triple[2])\n",
    "        out_graph.add((URIRef(s), URIRef(p), URIRef(o)))\n",
    "\n",
    "    out_graph.serialize(path.with_suffix(\".nt\"), format=\"nt\")\n",
    "\n",
    "!cat {dataset_path}/abox/splits/*.nt > {dataset_path}/abox/triples.nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del out_graph\n",
    "del data_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741220b6",
   "metadata": {},
   "source": [
    "# ABOX Individuals and Class Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_triples = Graph()\n",
    "data_triples.parse(dataset_path / \"abox\" / \"triples.nt\")\n",
    "\n",
    "individuals = set(data_triples.subjects()) | set(data_triples.objects())\n",
    "\n",
    "print(\"Len Individuals\", len(individuals))\n",
    "del data_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec37c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_graph = Graph()\n",
    "\n",
    "for ind in individuals:\n",
    "    out_graph.add((ind, RDF.type, OWL.NamedIndividual))\n",
    "\n",
    "serialize(out_graph, dataset_path / \"abox\" / \"individuals\")\n",
    "del out_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_graph = Graph()\n",
    "ca_graph.parse(onto_path / \"ApuliaTravelABox.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aff20c",
   "metadata": {},
   "source": [
    "### [BASE] RDF Lib Class Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_graph = Graph()\n",
    "\n",
    "for ind in individuals:\n",
    "    for ca in  set(ca_graph.objects(ind, RDF.type)) - BUILTIN_URI:\n",
    "        if (ca, RDF.type, OWL.Class) in apulia_ontology:\n",
    "            out_graph.add((ind, RDF.type, ca))\n",
    "        else:\n",
    "            print(f\"Not a Class {ca}\")\n",
    "\n",
    "serialize(out_graph, dataset_path / \"abox\" / \"class_assertions\")\n",
    "del out_graph\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928eb715",
   "metadata": {},
   "source": [
    "### [REASONED] Reasoner Class Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_graph = Graph()\n",
    "\n",
    "\n",
    "for ind in individuals:\n",
    "    for ca in  set(ca_graph.objects(ind, RDF.type)) - BUILTIN_URI:\n",
    "        if (ca, RDF.type, OWL.Class) in apulia_ontology:\n",
    "            out_graph.add((ind, RDF.type, ca))\n",
    "        else:\n",
    "            print(f\"Not a Class {ca}\")\n",
    "\n",
    "serialize(out_graph, dataset_path / \"abox\" / \"unreasoned_class_assertions\")\n",
    "del out_graph\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb94a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -Xmx16G -jar /home/navis/robot/robot.jar merge -vvv \\\n",
    "    --input {dataset_path / \"abox\" / \"unreasoned_class_assertions.owl\"} \\\n",
    "    --input {dataset_path / \"abox\" / \"individuals.owl\"} \\\n",
    "    --input {dataset_path / \"abox\" / \"triples.nt\"} \\\n",
    "    --input {apulia_path / \"apulia_travel_merged_materialized.owl\"} \\\n",
    "    --output {dataset_path / \"abox\" / \"intermediate_abox_tbox.owl\"}\n",
    "\n",
    "!java -Xmx16G -jar /home/navis/robot/robot.jar reason -vvv \\\n",
    "  --reasoner HermiT \\\n",
    "  --create-new-ontology true \\\n",
    "  --input {dataset_path / \"abox\" / \"intermediate_abox_tbox.owl\"} \\\n",
    "  --output {dataset_path / \"abox\" / \"inferred_class_assertions.owl\"} \\\n",
    "  --axiom-generators \"ClassAssertion\" \\\n",
    "  --remove-redundant-subclass-axioms false \\\n",
    "  --exclude-tautologies structural \\\n",
    "  --include-indirect true \\\n",
    "  -D {dataset_path / \"class_assertions_debug.owl\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = Graph()\n",
    "ca.parse(dataset_path / \"abox\" / \"unreasoned_class_assertions.owl\")\n",
    "ca.parse(dataset_path / \"abox\" / \"inferred_class_assertions.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_graph = Graph()\n",
    "\n",
    "for ind in individuals:\n",
    "    for o in set(ca.objects(ind, RDF.type)) - BUILTIN_URI:\n",
    "        out_graph.add((ind,RDF.type, o))\n",
    "\n",
    "serialize(out_graph, dataset_path / \"abox\" / \"class_assertions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_path / \"abox\" / \"inferred_class_assertions.owl\").unlink()\n",
    "(dataset_path / \"abox\" / \"unreasoned_class_assertions.owl\").unlink()\n",
    "(dataset_path / \"abox\" / \"intermediate_abox_tbox.owl\").unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab44efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(out_graph)\n",
    "del(ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6959a9",
   "metadata": {},
   "source": [
    "# TBOX and RBOX Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from rdflib import Graph, URIRef, BNode, RDF, RDFS, OWL\n",
    "import  utils.conventions.paths as pc\n",
    "from utils.conventions.builtins import BUILTIN_URIS\n",
    "\n",
    "class SignatureModularizer:\n",
    "    def __init__(self, schema, seed):\n",
    "        self.schema = schema\n",
    "        self.seed = seed\n",
    "\n",
    "    def modularize(self):\n",
    "        return self._extract_recursive_description()\n",
    "\n",
    "    def _extract_recursive_description(self) -> Graph:\n",
    "\n",
    "        extracted_graph = Graph()\n",
    "        elem_to_process = set(self.seed)\n",
    "        processed = set()\n",
    "\n",
    "        while elem_to_process:\n",
    "\n",
    "            e = elem_to_process.pop()\n",
    "            processed.add(e)\n",
    "\n",
    "            print(f\"Processing {e}\")\n",
    "\n",
    "            for s,p,o in self.schema.triples((e, None, None)):\n",
    "                extracted_graph.add((s,p,o))\n",
    "\n",
    "                if (o not in BUILTIN_URIS) and (o not in processed):\n",
    "\n",
    "                    if isinstance(o, BNode):\n",
    "                        elem_to_process.add(o)\n",
    "\n",
    "                    if (o, RDF.type, OWL.Class) in self.schema:\n",
    "                        elem_to_process.add(o)\n",
    "\n",
    "                    if (o, RDF.type, OWL.ObjectProperty) in self.schema:\n",
    "                        elem_to_process.add(o)\n",
    "\n",
    "                    if (o, RDF.type, OWL.DatatypeProperty) in self.schema:\n",
    "                        elem_to_process.add(o)\n",
    "\n",
    "        return extracted_graph\n",
    "    \n",
    "\n",
    "class SchemaDecomposition:\n",
    "    def __init__(self, input_graph):\n",
    "        self.onto_graph = input_graph\n",
    "    \n",
    "    def decompose(self):\n",
    "        return self._rbox_decompose(), self._taxonomy_decompose(), self._schema_decompose()\n",
    "\n",
    "\n",
    "    def _rbox_decompose(self):\n",
    "        rbox_graph = Graph()\n",
    "        for prop in set(self.onto_graph.subjects(RDF.type, OWL.ObjectProperty)) - BUILTIN_URI:\n",
    "            rbox_graph += self._extract_description(prop)\n",
    "\n",
    "        for prop in set(self.onto_graph.subjects(RDF.type, OWL.DatatypeProperty)) - BUILTIN_URI:\n",
    "            rbox_graph += self._extract_description(prop)\n",
    "        return rbox_graph\n",
    "\n",
    "\n",
    "    def _taxonomy_decompose(self):\n",
    "        taxonomy_graph = Graph()\n",
    "\n",
    "        for c in set(self.onto_graph.subjects(RDF.type, OWL.Class)) - BUILTIN_URI:\n",
    "            for s,p,o in self.onto_graph.triples((c, None, None)):\n",
    "                if p == RDFS.subClassOf:\n",
    "                    taxonomy_graph.add((s,p,o))\n",
    "                    if isinstance(o, BNode):\n",
    "                        taxonomy_graph += self._extract_description(o)\n",
    "\n",
    "        return taxonomy_graph\n",
    "\n",
    "    def _schema_decompose(self):\n",
    "        schema_graph = Graph()\n",
    "\n",
    "        for c in set(self.onto_graph.subjects(RDF.type, OWL.Class)) - BUILTIN_URI:\n",
    "            if not isinstance(c, BNode):\n",
    "                for s,p,o in self.onto_graph.triples((c, None, None)):\n",
    "                    if p != RDFS.subClassOf:\n",
    "                        \n",
    "                        schema_graph.add((s,p,o))\n",
    "\n",
    "                        for elem in self.onto_graph.objects(o, RDF.type):\n",
    "                            schema_graph.add((o, RDF.type, elem))\n",
    "\n",
    "                        if isinstance(o, BNode):\n",
    "                            print(f\"Found BNODE in Triple {s, p, o}\")\n",
    "                            schema_graph += self._extract_description(o)\n",
    "\n",
    "        return schema_graph\n",
    "\n",
    "    def _extract_description(self, elem: URIRef) -> Graph:\n",
    "\n",
    "        extracted_graph = Graph()\n",
    "        elem_to_process = {elem}\n",
    "        processed = set()\n",
    "\n",
    "        while elem_to_process:\n",
    "\n",
    "            e = elem_to_process.pop()\n",
    "            processed.add(e)\n",
    "\n",
    "            print(f\"Processing {e}\")\n",
    "\n",
    "            for s,p,o in self.onto_graph.triples((e, None, None)):\n",
    "                extracted_graph.add((s,p,o))\n",
    "\n",
    "                if (o not in BUILTIN_URI) and (o not in processed):\n",
    "                    if isinstance(o, BNode):\n",
    "                        elem_to_process.add(o)\n",
    "\n",
    "                    if (o, RDF.type, OWL.Class) in self.onto_graph:\n",
    "                        extracted_graph.add((o, RDF.type, OWL.Class))\n",
    "\n",
    "                    if (o, RDF.type, OWL.ObjectProperty) in self.onto_graph:\n",
    "                        extracted_graph.add((o, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "                    if (o, RDF.type, OWL.DatatypeProperty) in self.onto_graph:\n",
    "                        extracted_graph.add((o, RDF.type, OWL.DatatypeProperty))\n",
    "\n",
    "        return extracted_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_triples = Graph()\n",
    "data_triples.parse(dataset_path / \"abox\" / \"triples.nt\")\n",
    "\n",
    "class_assertions = Graph()\n",
    "class_assertions.parse(dataset_path / \"abox\" / \"class_assertions.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_obj_props = set(data_triples.predicates())\n",
    "print(\"Seed Object Properties\", len(seed_obj_props))\n",
    "\n",
    "seed_classes =  set(class_assertions.subjects(RDF.type, OWL.Class))\n",
    "print(\"Seed Classes\", len(seed_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modularizer = SignatureModularizer(apulia_ontology, seed_classes | seed_obj_props)\n",
    "out_graph = modularizer.modularize()\n",
    "\n",
    "serialize(out_graph, dataset_path / \"ontology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3624f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_graph = Graph()\n",
    "onto_graph.parse(dataset_path / \"ontology.owl\")\n",
    "\n",
    "decomposer = SchemaDecomposition(onto_graph)\n",
    "rbox_graph, taxonomy_graph, schema_graph = decomposer.decompose()\n",
    "\n",
    "serialize(rbox_graph, dataset_path / \"rbox\" / \"roles\")\n",
    "serialize(taxonomy_graph, dataset_path / \"tbox\" / \"taxonomy\")\n",
    "serialize(schema_graph, dataset_path / \"tbox\" / \"schema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import BNode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_description(graph: Graph, elem: URIRef) -> Graph:\n",
    "\n",
    "    extracted_graph = Graph()\n",
    "    elem_to_process = {elem}\n",
    "    processed = set()\n",
    "\n",
    "\n",
    "    while elem_to_process:\n",
    "\n",
    "        e = elem_to_process.pop()\n",
    "        processed.add(e)\n",
    "\n",
    "        print(f\"Processing {e}\")\n",
    "\n",
    "        for s,p,o in graph.triples((e, None, None)):\n",
    "            extracted_graph.add((s,p,o))\n",
    "\n",
    "            if (o not in BUILTIN_URI) and (o not in processed):\n",
    "                if isinstance(o, BNode):\n",
    "                    elem_to_process.add(o)\n",
    "\n",
    "                if (o, RDF.type, OWL.Class) in graph:\n",
    "                    extracted_graph.add((o, RDF.type, OWL.Class))\n",
    "\n",
    "                if (o, RDF.type, OWL.ObjectProperty) in graph:\n",
    "                    extracted_graph.add((o, RDF.type, OWL.ObjectProperty))\n",
    "\n",
    "                if (o, RDF.type, OWL.DatatypeProperty) in graph:\n",
    "                    extracted_graph.add((o, RDF.type, OWL.DatatypeProperty))\n",
    "\n",
    "    return extracted_graph\n",
    "\n",
    "\n",
    "rbox_graph = Graph()\n",
    "for prop in set(onto_graph.subjects(RDF.type, OWL.ObjectProperty)) - BUILTIN_URI:\n",
    "    rbox_graph += extract_description(onto_graph, prop)\n",
    "\n",
    "for prop in set(onto_graph.subjects(RDF.type, OWL.DatatypeProperty)) - BUILTIN_URI:\n",
    "    rbox_graph += extract_description(onto_graph, prop)\n",
    "\n",
    "\n",
    "serialize(rbox_graph, dataset_path / \"rbox\" / \"roles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_graph = Graph()\n",
    "\n",
    "for c in set(onto_graph.subjects(RDF.type, OWL.Class)) - BUILTIN_URI:\n",
    "    for s,p,o in onto_graph.triples((c, None, None)):\n",
    "        if p == RDFS.subClassOf:\n",
    "            taxonomy_graph.add((s,p,o))\n",
    "            if isinstance(o, BNode):\n",
    "                taxonomy_graph += extract_description(onto_graph, o)\n",
    "\n",
    "serialize(taxonomy_graph, dataset_path / \"tbox\" / \"taxonomy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_graph = Graph()\n",
    "\n",
    "\n",
    "for c in set(onto_graph.subjects(RDF.type, OWL.Class)) - BUILTIN_URI:\n",
    "    if not isinstance(c, BNode):\n",
    "        for s,p,o in onto_graph.triples((c, None, None)):\n",
    "            if p != RDFS.subClassOf:\n",
    "                \n",
    "                schema_graph.add((s,p,o))\n",
    "\n",
    "                for elem in onto_graph.objects(o, RDF.type):\n",
    "                    schema_graph.add((o, RDF.type, elem))\n",
    "\n",
    "                if isinstance(o, BNode):\n",
    "                    print(f\"Found BNODE in Triple {s, p, o}\")\n",
    "                    schema_graph += extract_description(onto_graph, o)\n",
    "            \n",
    "\n",
    "serialize(schema_graph, dataset_path / \"tbox\" / \"schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03922d4f",
   "metadata": {},
   "source": [
    "# Final Ontology and Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/navis/robot/robot merge \\\n",
    "--input  {dataset_path / \"ontology.owl\"} \\\n",
    "--input  {dataset_path / \"abox\" / \"individuals.owl\"} \\\n",
    "--input {dataset_path / \"abox\" / \"triples.nt\"} \\\n",
    "--input {dataset_path / \"abox\" / \"class_assertions.owl\"} \\\n",
    "--output {dataset_path / \"knowledge_graph.owl\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
